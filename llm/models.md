# LLM Basics 

Maintainers - [Aaron Zhao](https://aaron-zhao123.github.io/)

A list of popular LLM network architectures LLM architectures.


|  Title  |   Year   |   Topic  |   Venue  |   Code/Implementation   |   Notes  |
|:--------|:--------:|:--------:|:--------:|:--------:|:--------:|
| [Attention Is All You Need](<https://arxiv.org/abs/1706.03762>) | 2017 | Architecture | NeurIPS | - | Classic 6-layer architecture |
| [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](<https://arxiv.org/abs/1810.04805>) | 2019 | Architecture | ACL | [Huggingface](https://huggingface.co/docs/transformers/v4.36.1/en/model_doc/bert#overview) | BERT (encoder-decoder) |
| [RoBERTa: A Robustly Optimized BERT Pretraining Approach](<https://arxiv.org/abs/1907.11692>) | 2019 | Architecture | ACL | [Huggingface](https://huggingface.co/docs/transformers/model_doc/roberta) | RoBERTa (encoder-decoder) |
| [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](<https://arxiv.org/abs/1910.10683>) | 2020 | Architecture | ICML | [Huggingface](https://huggingface.co/docs/transformers/model_doc/t5) | T5 (encoder-decoder) |
| [Language Models are Few-Shot Learners](<https://arxiv.org/abs/2005.14165>) | 2020 | Architecture | NeurIPS | - |  GPT series (decoder only) |
| [mT5: A massively multilingual pre-trained text-to-text transformer](<https://arxiv.org/abs/2010.11934>) | 2021 | Architecture | ACL | [Huggingface](https://huggingface.co/docs/transformers/v4.36.1/en/model_doc/mt5#overview) | mT5 (encoder-decoder) |
| [Scaling Instruction-Finetuned Language Models](<https://arxiv.org/abs/2210.11416>) | 2022 | Architecture | - | [Huggingface](https://huggingface.co/docs/transformers/model_doc/flan-t5) | FLAN (encoder-decoder) |
| [Open Pre-trained Transformer Language Models](<https://arxiv.org/abs/2210.11416>) | 2022 | Architecture | - | [Huggingface](https://huggingface.co/docs/transformers/model_doc/flan-t5) | OPT (decoder only) |
